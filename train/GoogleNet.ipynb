{"cells":[{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1954,"status":"ok","timestamp":1704835519739,"user":{"displayName":"Jean Carlos Soncco Lupa","userId":"15843525091642984262"},"user_tz":300},"id":"9u9_rpVGZc4q","outputId":"fe7a42f5-9c25-405b-ed9b-6513946b65eb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":547,"status":"ok","timestamp":1704835520279,"user":{"displayName":"Jean Carlos Soncco Lupa","userId":"15843525091642984262"},"user_tz":300},"id":"Mx1ZAfNva39P","outputId":"4ef11f17-0907-495b-99b9-371bb5b0863f"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/proyectos3\n","alexNet.ipynb  efficientNet.ipynb  GoogleNet.ipynb  PlantDoc\t\t    resNet.ipynb\n","cnn.ipynb      evaluation\t   models\t    PlantVillage_bgremoved  wandb\n"]}],"source":["%cd /content/drive/MyDrive/proyectos3/\n","!ls"]},{"cell_type":"markdown","metadata":{"id":"QCZO4-Wz9Mi5"},"source":["# Cargar Conjunto de Datos"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1704835520280,"user":{"displayName":"Jean Carlos Soncco Lupa","userId":"15843525091642984262"},"user_tz":300},"id":"o2PjUn8wbFMF","outputId":"6b06f5d2-6b31-4fab-b5f0-b8ba6ffb28fd"},"outputs":[{"data":{"text/plain":["['Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy']"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import ConcatDataset\n","\n","\n","train_dir0 = './PlantVillage_bgremoved/train'\n","train_dir1 = './PlantDoc/train/'\n","\n","val_dir0 = './PlantVillage_bgremoved/valid'\n","val_dir1 = './PlantDoc/test/'\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","train_dataset0 = ImageFolder(train_dir0, transform=transform,  is_valid_file=lambda x: x.endswith('.JPG'))\n","train_dataset1 = ImageFolder(train_dir1, transform=transform)\n","\n","val_dataset0 = ImageFolder(val_dir0, transform=transform, is_valid_file=lambda x: x.endswith('.JPG'))\n","val_dataset1 = ImageFolder(val_dir1, transform=transform)\n","\n","train_dataset = ConcatDataset([train_dataset0, train_dataset1])\n","val_dataset = ConcatDataset([val_dataset0, val_dataset1])\n","\n","classes = train_dataset0.classes\n","\n","classes"]},{"cell_type":"markdown","metadata":{"id":"ZugaulR0QBux"},"source":["crear carga de datos para facilitar el acceso a los datos"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1704835520280,"user":{"displayName":"Jean Carlos Soncco Lupa","userId":"15843525091642984262"},"user_tz":300},"id":"5rednLcxPyaL"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","batch_size = 32\n","num_workers = 8 # Número de procesos que se utilizarán para cargar datos en paralelo\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,pin_memory=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1704835520280,"user":{"displayName":"Jean Carlos Soncco Lupa","userId":"15843525091642984262"},"user_tz":300},"id":"hSnPPujFQ282"},"outputs":[],"source":["import torch\n","\n","def get_default_device():\n","    if torch.cuda.is_available():\n","        return torch.device('cuda')\n","    else:\n","        return torch.device('cpu')\n","\n","def to_device(data, device):\n","    if isinstance(data, (list,tuple)):\n","        return [to_device(x, device) for x in data]\n","    return data.to(device, non_blocking=True)\n","\n","class DeviceDataLoader():\n","    def __init__(self, dl, device):\n","        self.dl = dl\n","        self.device = device\n","\n","    def __iter__(self):\n","        for b in self.dl:\n","            yield to_device(b, self.device)\n","\n","    def __len__(self):\n","        return len(self.dl)"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1704835520280,"user":{"displayName":"Jean Carlos Soncco Lupa","userId":"15843525091642984262"},"user_tz":300},"id":"S6TcjHtjR0F_"},"outputs":[],"source":["device = get_default_device()\n","\n","train_loader = DeviceDataLoader(train_loader, device)\n","val_loader = DeviceDataLoader(val_loader, device)"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1704835520280,"user":{"displayName":"Jean Carlos Soncco Lupa","userId":"15843525091642984262"},"user_tz":300},"id":"k6XLy8mIIeSy","outputId":"2b2367c4-ef25-4ffe-d330-a945a44e0e35"},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["device"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1704835520281,"user":{"displayName":"Jean Carlos Soncco Lupa","userId":"15843525091642984262"},"user_tz":300},"id":"MWuOy6AJrrnH"},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn.metrics import precision_score, f1_score\n","\n","def accuracy(outputs, labels):\n","    _, preds = torch.max(outputs, dim=1)\n","    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n","\n","\n","class ImageClassificationBase(nn.Module):\n","\n","    def training_step(self, batch):\n","        \"calculate loss for a batch of training data\"\n","        images, labels = batch\n","        out = self(images)                  # Generate predictions\n","        # nn.CrossEntropyLoss combines nn.LogSoftmax and nn.NLLLoss\n","        loss = F.cross_entropy(out, labels) # Calculate loss\n","        return loss\n","\n","    def validation_step(self, batch):\n","        \"calculate loss, accuracy, precision and f1 score for a batch of validation data\"\n","        images, labels = batch\n","        out = self(images)                   # Generate prediction\n","        # nn.CrossEntropyLoss combines nn.LogSoftmax and nn.NLLLoss\n","        loss = F.cross_entropy(out, labels)  # Calculate loss\n","        preds = torch.argmax(out, dim=1)\n","        acc = accuracy(out, labels)          # Calculate accuracy\n","\n","        # calculate precision and f1 score\n","        precision = precision_score(labels.cpu(), preds.cpu(), average='weighted', zero_division=1)\n","        f1 = f1_score(labels.cpu(), preds.cpu(), average='macro')\n","        return {'val_loss': loss.detach(), 'val_accuracy': acc, 'val_precision': precision, 'val_f1': f1}\n","\n","    def validation_epoch_end(self, outputs):\n","        batch_losses = [x[\"val_loss\"] for x in outputs]\n","        epoch_loss = torch.stack(batch_losses).mean()       # Combine loss\n","        batch_accuracy = [x[\"val_accuracy\"] for x in outputs]\n","        epoch_accuracy = torch.stack(batch_accuracy).mean()\n","        return {\"val_loss\": epoch_loss.item(), \"val_accuracy\": epoch_accuracy.item()} # Combine accuracies\n","\n","    def epoch_end(self, epoch, result):\n","        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n","            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_accuracy']))\n"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1704835520281,"user":{"displayName":"Jean Carlos Soncco Lupa","userId":"15843525091642984262"},"user_tz":300},"id":"WLTn1g_nsrRn"},"outputs":[],"source":["def BasicConv2d(in_channels, out_channels, **kwargs):\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, out_channels, bias=False, **kwargs),\n","        nn.BatchNorm2d(out_channels, eps=0.001),\n","        nn.ReLU(inplace=True)\n","    )\n","# Inception module\n","class Inception(nn.Module):\n","    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj):\n","        super(Inception, self).__init__()\n","\n","        # 1x1 conv branch\n","        self.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=1)\n","\n","        # 1x1 conv -> 3x3 conv branch\n","        self.branch2 = nn.Sequential(\n","            BasicConv2d(in_channels, ch3x3red, kernel_size=1),\n","            BasicConv2d(ch3x3red, ch3x3, kernel_size=3, padding=1)\n","        )\n","\n","        # 1x1 conv -> 5x5 conv branch\n","        self.branch3 = nn.Sequential(\n","            BasicConv2d(in_channels, ch5x5red, kernel_size=1),\n","            BasicConv2d(ch5x5red, ch5x5, kernel_size=5, padding=2)\n","        )\n","\n","        # 3x3 pool -> 1x1 conv branch\n","        self.branch4 = nn.Sequential(\n","            nn.MaxPool2d(kernel_size=3, stride=1, padding=1, ceil_mode=True),\n","            BasicConv2d(in_channels, pool_proj, kernel_size=1)\n","        )\n","\n","    def forward(self, x):\n","        x1 = self.branch1(x)\n","        x2 = self.branch2(x)\n","        x3 = self.branch3(x)\n","        x4 = self.branch4(x)\n","\n","        # concat along channel axis\n","        return torch.cat([x1, x2, x3, x4], 1)\n","\n","# GoogLeNet\n","class GoogLeNet(ImageClassificationBase):\n","    def __init__(self, num_classes=38):\n","        super(GoogLeNet, self).__init__()\n","\n","        self.conv1 = BasicConv2d(3, 64, kernel_size=7, stride=2, padding=3)\n","        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n","\n","        self.conv2 = BasicConv2d(64, 64, kernel_size=1)\n","        self.conv3 = BasicConv2d(64, 192, kernel_size=3, padding=1)\n","        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n","\n","        self.inception3a = Inception(192, 64, 96, 128, 16, 32, 32)\n","        self.inception3b = Inception(256, 128, 128, 192, 32, 96, 64)\n","        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n","\n","        self.inception4a = Inception(480, 192, 96, 208, 16, 48, 64)\n","        self.inception4b = Inception(512, 160, 112, 224, 24, 64, 64)\n","        self.inception4c = Inception(512, 128, 128, 256, 24, 64, 64)\n","        self.inception4d = Inception(512, 112, 144, 288, 32, 64, 64)\n","        self.inception4e = Inception(528, 256, 160, 320, 32, 128, 128)\n","        self.maxpool4 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n","\n","        self.inception5a = Inception(832, 256, 160, 320, 32, 128, 128)\n","        self.inception5b = Inception(832, 384, 192, 384, 48, 128, 128)\n","\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.dropout = nn.Dropout(0.4)\n","        self.fc = nn.Linear(1024, num_classes)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.maxpool1(x)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","        x = self.maxpool2(x)\n","\n","        x = self.inception3a(x)\n","        x = self.inception3b(x)\n","        x = self.maxpool3(x)\n","\n","        x = self.inception4a(x)\n","        x = self.inception4b(x)\n","        x = self.inception4c(x)\n","        x = self.inception4d(x)\n","        x = self.inception4e(x)\n","        x = self.maxpool4(x)\n","\n","        x = self.inception5a(x)\n","        x = self.inception5b(x)\n","\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.dropout(x)\n","        x = self.fc(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":297,"status":"ok","timestamp":1704835520571,"user":{"displayName":"Jean Carlos Soncco Lupa","userId":"15843525091642984262"},"user_tz":300},"id":"EBrx9WYxstKo"},"outputs":[],"source":["model = GoogLeNet(len(classes))\n","model = to_device(model, device)"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":359,"status":"ok","timestamp":1704835520927,"user":{"displayName":"Jean Carlos Soncco Lupa","userId":"15843525091642984262"},"user_tz":300},"id":"CMuflAwMs0TH","outputId":"370aa60a-d74c-40a6-d805-3e54cfdb8569"},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 112, 112]           9,408\n","       BatchNorm2d-2         [-1, 64, 112, 112]             128\n","              ReLU-3         [-1, 64, 112, 112]               0\n","         MaxPool2d-4           [-1, 64, 56, 56]               0\n","            Conv2d-5           [-1, 64, 56, 56]           4,096\n","       BatchNorm2d-6           [-1, 64, 56, 56]             128\n","              ReLU-7           [-1, 64, 56, 56]               0\n","            Conv2d-8          [-1, 192, 56, 56]         110,592\n","       BatchNorm2d-9          [-1, 192, 56, 56]             384\n","             ReLU-10          [-1, 192, 56, 56]               0\n","        MaxPool2d-11          [-1, 192, 28, 28]               0\n","           Conv2d-12           [-1, 64, 28, 28]          12,288\n","      BatchNorm2d-13           [-1, 64, 28, 28]             128\n","             ReLU-14           [-1, 64, 28, 28]               0\n","           Conv2d-15           [-1, 96, 28, 28]          18,432\n","      BatchNorm2d-16           [-1, 96, 28, 28]             192\n","             ReLU-17           [-1, 96, 28, 28]               0\n","           Conv2d-18          [-1, 128, 28, 28]         110,592\n","      BatchNorm2d-19          [-1, 128, 28, 28]             256\n","             ReLU-20          [-1, 128, 28, 28]               0\n","           Conv2d-21           [-1, 16, 28, 28]           3,072\n","      BatchNorm2d-22           [-1, 16, 28, 28]              32\n","             ReLU-23           [-1, 16, 28, 28]               0\n","           Conv2d-24           [-1, 32, 28, 28]          12,800\n","      BatchNorm2d-25           [-1, 32, 28, 28]              64\n","             ReLU-26           [-1, 32, 28, 28]               0\n","        MaxPool2d-27          [-1, 192, 28, 28]               0\n","           Conv2d-28           [-1, 32, 28, 28]           6,144\n","      BatchNorm2d-29           [-1, 32, 28, 28]              64\n","             ReLU-30           [-1, 32, 28, 28]               0\n","        Inception-31          [-1, 256, 28, 28]               0\n","           Conv2d-32          [-1, 128, 28, 28]          32,768\n","      BatchNorm2d-33          [-1, 128, 28, 28]             256\n","             ReLU-34          [-1, 128, 28, 28]               0\n","           Conv2d-35          [-1, 128, 28, 28]          32,768\n","      BatchNorm2d-36          [-1, 128, 28, 28]             256\n","             ReLU-37          [-1, 128, 28, 28]               0\n","           Conv2d-38          [-1, 192, 28, 28]         221,184\n","      BatchNorm2d-39          [-1, 192, 28, 28]             384\n","             ReLU-40          [-1, 192, 28, 28]               0\n","           Conv2d-41           [-1, 32, 28, 28]           8,192\n","      BatchNorm2d-42           [-1, 32, 28, 28]              64\n","             ReLU-43           [-1, 32, 28, 28]               0\n","           Conv2d-44           [-1, 96, 28, 28]          76,800\n","      BatchNorm2d-45           [-1, 96, 28, 28]             192\n","             ReLU-46           [-1, 96, 28, 28]               0\n","        MaxPool2d-47          [-1, 256, 28, 28]               0\n","           Conv2d-48           [-1, 64, 28, 28]          16,384\n","      BatchNorm2d-49           [-1, 64, 28, 28]             128\n","             ReLU-50           [-1, 64, 28, 28]               0\n","        Inception-51          [-1, 480, 28, 28]               0\n","        MaxPool2d-52          [-1, 480, 14, 14]               0\n","           Conv2d-53          [-1, 192, 14, 14]          92,160\n","      BatchNorm2d-54          [-1, 192, 14, 14]             384\n","             ReLU-55          [-1, 192, 14, 14]               0\n","           Conv2d-56           [-1, 96, 14, 14]          46,080\n","      BatchNorm2d-57           [-1, 96, 14, 14]             192\n","             ReLU-58           [-1, 96, 14, 14]               0\n","           Conv2d-59          [-1, 208, 14, 14]         179,712\n","      BatchNorm2d-60          [-1, 208, 14, 14]             416\n","             ReLU-61          [-1, 208, 14, 14]               0\n","           Conv2d-62           [-1, 16, 14, 14]           7,680\n","      BatchNorm2d-63           [-1, 16, 14, 14]              32\n","             ReLU-64           [-1, 16, 14, 14]               0\n","           Conv2d-65           [-1, 48, 14, 14]          19,200\n","      BatchNorm2d-66           [-1, 48, 14, 14]              96\n","             ReLU-67           [-1, 48, 14, 14]               0\n","        MaxPool2d-68          [-1, 480, 14, 14]               0\n","           Conv2d-69           [-1, 64, 14, 14]          30,720\n","      BatchNorm2d-70           [-1, 64, 14, 14]             128\n","             ReLU-71           [-1, 64, 14, 14]               0\n","        Inception-72          [-1, 512, 14, 14]               0\n","           Conv2d-73          [-1, 160, 14, 14]          81,920\n","      BatchNorm2d-74          [-1, 160, 14, 14]             320\n","             ReLU-75          [-1, 160, 14, 14]               0\n","           Conv2d-76          [-1, 112, 14, 14]          57,344\n","      BatchNorm2d-77          [-1, 112, 14, 14]             224\n","             ReLU-78          [-1, 112, 14, 14]               0\n","           Conv2d-79          [-1, 224, 14, 14]         225,792\n","      BatchNorm2d-80          [-1, 224, 14, 14]             448\n","             ReLU-81          [-1, 224, 14, 14]               0\n","           Conv2d-82           [-1, 24, 14, 14]          12,288\n","      BatchNorm2d-83           [-1, 24, 14, 14]              48\n","             ReLU-84           [-1, 24, 14, 14]               0\n","           Conv2d-85           [-1, 64, 14, 14]          38,400\n","      BatchNorm2d-86           [-1, 64, 14, 14]             128\n","             ReLU-87           [-1, 64, 14, 14]               0\n","        MaxPool2d-88          [-1, 512, 14, 14]               0\n","           Conv2d-89           [-1, 64, 14, 14]          32,768\n","      BatchNorm2d-90           [-1, 64, 14, 14]             128\n","             ReLU-91           [-1, 64, 14, 14]               0\n","        Inception-92          [-1, 512, 14, 14]               0\n","           Conv2d-93          [-1, 128, 14, 14]          65,536\n","      BatchNorm2d-94          [-1, 128, 14, 14]             256\n","             ReLU-95          [-1, 128, 14, 14]               0\n","           Conv2d-96          [-1, 128, 14, 14]          65,536\n","      BatchNorm2d-97          [-1, 128, 14, 14]             256\n","             ReLU-98          [-1, 128, 14, 14]               0\n","           Conv2d-99          [-1, 256, 14, 14]         294,912\n","     BatchNorm2d-100          [-1, 256, 14, 14]             512\n","            ReLU-101          [-1, 256, 14, 14]               0\n","          Conv2d-102           [-1, 24, 14, 14]          12,288\n","     BatchNorm2d-103           [-1, 24, 14, 14]              48\n","            ReLU-104           [-1, 24, 14, 14]               0\n","          Conv2d-105           [-1, 64, 14, 14]          38,400\n","     BatchNorm2d-106           [-1, 64, 14, 14]             128\n","            ReLU-107           [-1, 64, 14, 14]               0\n","       MaxPool2d-108          [-1, 512, 14, 14]               0\n","          Conv2d-109           [-1, 64, 14, 14]          32,768\n","     BatchNorm2d-110           [-1, 64, 14, 14]             128\n","            ReLU-111           [-1, 64, 14, 14]               0\n","       Inception-112          [-1, 512, 14, 14]               0\n","          Conv2d-113          [-1, 112, 14, 14]          57,344\n","     BatchNorm2d-114          [-1, 112, 14, 14]             224\n","            ReLU-115          [-1, 112, 14, 14]               0\n","          Conv2d-116          [-1, 144, 14, 14]          73,728\n","     BatchNorm2d-117          [-1, 144, 14, 14]             288\n","            ReLU-118          [-1, 144, 14, 14]               0\n","          Conv2d-119          [-1, 288, 14, 14]         373,248\n","     BatchNorm2d-120          [-1, 288, 14, 14]             576\n","            ReLU-121          [-1, 288, 14, 14]               0\n","          Conv2d-122           [-1, 32, 14, 14]          16,384\n","     BatchNorm2d-123           [-1, 32, 14, 14]              64\n","            ReLU-124           [-1, 32, 14, 14]               0\n","          Conv2d-125           [-1, 64, 14, 14]          51,200\n","     BatchNorm2d-126           [-1, 64, 14, 14]             128\n","            ReLU-127           [-1, 64, 14, 14]               0\n","       MaxPool2d-128          [-1, 512, 14, 14]               0\n","          Conv2d-129           [-1, 64, 14, 14]          32,768\n","     BatchNorm2d-130           [-1, 64, 14, 14]             128\n","            ReLU-131           [-1, 64, 14, 14]               0\n","       Inception-132          [-1, 528, 14, 14]               0\n","          Conv2d-133          [-1, 256, 14, 14]         135,168\n","     BatchNorm2d-134          [-1, 256, 14, 14]             512\n","            ReLU-135          [-1, 256, 14, 14]               0\n","          Conv2d-136          [-1, 160, 14, 14]          84,480\n","     BatchNorm2d-137          [-1, 160, 14, 14]             320\n","            ReLU-138          [-1, 160, 14, 14]               0\n","          Conv2d-139          [-1, 320, 14, 14]         460,800\n","     BatchNorm2d-140          [-1, 320, 14, 14]             640\n","            ReLU-141          [-1, 320, 14, 14]               0\n","          Conv2d-142           [-1, 32, 14, 14]          16,896\n","     BatchNorm2d-143           [-1, 32, 14, 14]              64\n","            ReLU-144           [-1, 32, 14, 14]               0\n","          Conv2d-145          [-1, 128, 14, 14]         102,400\n","     BatchNorm2d-146          [-1, 128, 14, 14]             256\n","            ReLU-147          [-1, 128, 14, 14]               0\n","       MaxPool2d-148          [-1, 528, 14, 14]               0\n","          Conv2d-149          [-1, 128, 14, 14]          67,584\n","     BatchNorm2d-150          [-1, 128, 14, 14]             256\n","            ReLU-151          [-1, 128, 14, 14]               0\n","       Inception-152          [-1, 832, 14, 14]               0\n","       MaxPool2d-153            [-1, 832, 7, 7]               0\n","          Conv2d-154            [-1, 256, 7, 7]         212,992\n","     BatchNorm2d-155            [-1, 256, 7, 7]             512\n","            ReLU-156            [-1, 256, 7, 7]               0\n","          Conv2d-157            [-1, 160, 7, 7]         133,120\n","     BatchNorm2d-158            [-1, 160, 7, 7]             320\n","            ReLU-159            [-1, 160, 7, 7]               0\n","          Conv2d-160            [-1, 320, 7, 7]         460,800\n","     BatchNorm2d-161            [-1, 320, 7, 7]             640\n","            ReLU-162            [-1, 320, 7, 7]               0\n","          Conv2d-163             [-1, 32, 7, 7]          26,624\n","     BatchNorm2d-164             [-1, 32, 7, 7]              64\n","            ReLU-165             [-1, 32, 7, 7]               0\n","          Conv2d-166            [-1, 128, 7, 7]         102,400\n","     BatchNorm2d-167            [-1, 128, 7, 7]             256\n","            ReLU-168            [-1, 128, 7, 7]               0\n","       MaxPool2d-169            [-1, 832, 7, 7]               0\n","          Conv2d-170            [-1, 128, 7, 7]         106,496\n","     BatchNorm2d-171            [-1, 128, 7, 7]             256\n","            ReLU-172            [-1, 128, 7, 7]               0\n","       Inception-173            [-1, 832, 7, 7]               0\n","          Conv2d-174            [-1, 384, 7, 7]         319,488\n","     BatchNorm2d-175            [-1, 384, 7, 7]             768\n","            ReLU-176            [-1, 384, 7, 7]               0\n","          Conv2d-177            [-1, 192, 7, 7]         159,744\n","     BatchNorm2d-178            [-1, 192, 7, 7]             384\n","            ReLU-179            [-1, 192, 7, 7]               0\n","          Conv2d-180            [-1, 384, 7, 7]         663,552\n","     BatchNorm2d-181            [-1, 384, 7, 7]             768\n","            ReLU-182            [-1, 384, 7, 7]               0\n","          Conv2d-183             [-1, 48, 7, 7]          39,936\n","     BatchNorm2d-184             [-1, 48, 7, 7]              96\n","            ReLU-185             [-1, 48, 7, 7]               0\n","          Conv2d-186            [-1, 128, 7, 7]         153,600\n","     BatchNorm2d-187            [-1, 128, 7, 7]             256\n","            ReLU-188            [-1, 128, 7, 7]               0\n","       MaxPool2d-189            [-1, 832, 7, 7]               0\n","          Conv2d-190            [-1, 128, 7, 7]         106,496\n","     BatchNorm2d-191            [-1, 128, 7, 7]             256\n","            ReLU-192            [-1, 128, 7, 7]               0\n","       Inception-193           [-1, 1024, 7, 7]               0\n","AdaptiveAvgPool2d-194           [-1, 1024, 1, 1]               0\n","         Dropout-195                 [-1, 1024]               0\n","          Linear-196                    [-1, 3]           3,075\n","================================================================\n","Total params: 5,983,907\n","Trainable params: 5,983,907\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 94.10\n","Params size (MB): 22.83\n","Estimated Total Size (MB): 117.50\n","----------------------------------------------------------------\n","None\n"]}],"source":["from torchsummary import summary              # for getting the summary of our model\n","\n","# getting summary of the model\n","INPUT_SHAPE = (3, 224, 224)\n","print(summary(model.cuda(), (INPUT_SHAPE)))"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1704835520927,"user":{"displayName":"Jean Carlos Soncco Lupa","userId":"15843525091642984262"},"user_tz":300},"id":"Z270ebzHs1Gf"},"outputs":[],"source":["epochs = 100\n","\n","max_lr = 0.01\n","grad_clip = 0.1\n","weight_decay = 1e-4\n","opt_func = torch.optim.Adam"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12923,"status":"ok","timestamp":1704835533838,"user":{"displayName":"Jean Carlos Soncco Lupa","userId":"15843525091642984262"},"user_tz":300},"id":"ayepRCIt_yZL","outputId":"9dc0cea7-5407-4dda-8ae0-b114c1d3e116"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.1)\n","Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.40)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.39.1)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"]}],"source":["!pip install wandb"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1704835533838,"user":{"displayName":"Jean Carlos Soncco Lupa","userId":"15843525091642984262"},"user_tz":300},"id":"L7oieSf5s5LY"},"outputs":[],"source":["import wandb\n","\n","name = f'googlenet-{epochs}-epochs'\n","wandb.init(project=\"plantvillage-aug\", name=name, config={\n","    \"epochs\": epochs,\n","    \"learning_rate\": max_lr,\n","    \"grad_clip\": grad_clip,\n","    \"weight_decay\": weight_decay,\n","    \"opt_func\": opt_func.__name__,\n","    \"batch_size\": batch_size,\n","    \"dataset\": \"PlantVillage\",\n","    \"architecture\": \"GoogleNet\"\n","})"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1704835533838,"user":{"displayName":"Jean Carlos Soncco Lupa","userId":"15843525091642984262"},"user_tz":300},"id":"OtS2oLwuuAx5"},"outputs":[],"source":["# for training\n","@torch.no_grad()\n","def evaluate(model, val_loader):\n","    \"\"\"Evaluates the model's performance on the validation set\"\"\"\n","    model.eval()\n","    outputs = [model.validation_step(batch) for batch in val_loader]\n","    # return model.validation_epoch_end(outputs)\n","    val_losses = [x[\"val_loss\"] for x in outputs]\n","    epoch_loss = torch.stack(val_losses).mean()       # Combine loss\n","    val_accuracies = [x[\"val_accuracy\"] for x in outputs]\n","    epoch_accuracy = torch.stack(val_accuracies).mean()\n","    val_precisions = [x[\"val_precision\"] for x in outputs]\n","    epoch_precision = torch.tensor(sum(val_precisions)/len(val_precisions))\n","    val_f1s = [x[\"val_f1\"] for x in outputs]\n","    epoch_f1 = torch.tensor(sum(val_f1s)/len(val_f1s))\n","    return {\"val_loss\": epoch_loss.item(), \"val_accuracy\": epoch_accuracy.item(), \"val_precision\": epoch_precision.item(), \"val_f1\": epoch_f1.item()} # Combine accuracies\n","\n","\n","def get_lr(optimizer):\n","    for param_group in optimizer.param_groups:\n","        return param_group['lr']\n","\n","\n","def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, weight_decay=0,\n","                grad_clip=None, opt_func=torch.optim.SGD):\n","    torch.cuda.empty_cache()\n","    history = []\n","\n","    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n","    # scheduler for one cycle learniing rate\n","    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, steps_per_epoch=len(train_loader))\n","\n","\n","    for epoch in range(epochs):\n","        # Training\n","        model.train()\n","        train_losses = []\n","        lrs = []\n","        for batch in train_loader:\n","            loss = model.training_step(batch)\n","            train_losses.append(loss)\n","            loss.backward()\n","\n","            # gradient clipping\n","            if grad_clip:\n","                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n","\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","            # recording and updating learning rates\n","            lrs.append(get_lr(optimizer))\n","            sched.step()\n","\n","            # logging to wandb\n","            wandb.log({\"Train Loss\": loss.item()})\n","\n","\n","        # validation\n","        result = evaluate(model, val_loader)\n","        result['train_loss'] = torch.stack(train_losses).mean().item()\n","        result['lrs'] = lrs\n","        model.epoch_end(epoch, result)\n","        history.append(result)\n","\n","        # logging to wandb\n","        wandb.log({\n","            \"Epoch\": epoch,\n","            \"Val Loss\": result['val_loss'],\n","            \"Val Accuracy\": result['val_accuracy'],\n","            \"Val Precision\": result['val_precision'],\n","            \"Val F1\": result['val_f1']\n","        })\n","\n","    return history"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2441519,"status":"ok","timestamp":1704837975352,"user":{"displayName":"Jean Carlos Soncco Lupa","userId":"15843525091642984262"},"user_tz":300},"id":"wQrurubluIFw","outputId":"039c013f-95df-4e36-c2ed-1afe04b85d4b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [0], last_lr: 0.00043, train_loss: 0.4831, val_loss: 0.2910, val_acc: 0.9013\n","Epoch [1], last_lr: 0.00050, train_loss: 0.2935, val_loss: 0.2884, val_acc: 0.8322\n","Epoch [2], last_lr: 0.00063, train_loss: 0.2124, val_loss: 0.3034, val_acc: 0.8849\n","Epoch [3], last_lr: 0.00081, train_loss: 0.2164, val_loss: 0.4166, val_acc: 0.8618\n","Epoch [4], last_lr: 0.00104, train_loss: 0.2298, val_loss: 0.2098, val_acc: 0.8717\n","Epoch [5], last_lr: 0.00131, train_loss: 0.2105, val_loss: 0.3847, val_acc: 0.8783\n","Epoch [6], last_lr: 0.00163, train_loss: 0.1966, val_loss: 0.3343, val_acc: 0.8454\n","Epoch [7], last_lr: 0.00198, train_loss: 0.2126, val_loss: 0.2328, val_acc: 0.9095\n","Epoch [8], last_lr: 0.00237, train_loss: 0.2083, val_loss: 0.4430, val_acc: 0.8289\n","Epoch [9], last_lr: 0.00280, train_loss: 0.1899, val_loss: 0.7965, val_acc: 0.7632\n","Epoch [10], last_lr: 0.00324, train_loss: 0.2328, val_loss: 1.4000, val_acc: 0.6875\n","Epoch [11], last_lr: 0.00371, train_loss: 0.2507, val_loss: 0.5188, val_acc: 0.7944\n","Epoch [12], last_lr: 0.00420, train_loss: 0.2140, val_loss: 0.3591, val_acc: 0.8586\n","Epoch [13], last_lr: 0.00469, train_loss: 0.1948, val_loss: 0.2763, val_acc: 0.8602\n","Epoch [14], last_lr: 0.00520, train_loss: 0.1873, val_loss: 0.1090, val_acc: 0.9490\n","Epoch [15], last_lr: 0.00570, train_loss: 0.2255, val_loss: 0.1854, val_acc: 0.9095\n","Epoch [16], last_lr: 0.00620, train_loss: 0.2029, val_loss: 0.2189, val_acc: 0.9095\n","Epoch [17], last_lr: 0.00668, train_loss: 0.1742, val_loss: 0.2277, val_acc: 0.8997\n","Epoch [18], last_lr: 0.00715, train_loss: 0.1892, val_loss: 0.1855, val_acc: 0.9408\n","Epoch [19], last_lr: 0.00760, train_loss: 0.2069, val_loss: 0.6899, val_acc: 0.8125\n","Epoch [20], last_lr: 0.00802, train_loss: 0.1625, val_loss: 0.1843, val_acc: 0.9260\n","Epoch [21], last_lr: 0.00841, train_loss: 0.1858, val_loss: 0.5698, val_acc: 0.7204\n","Epoch [22], last_lr: 0.00877, train_loss: 0.1740, val_loss: 0.1171, val_acc: 0.9589\n","Epoch [23], last_lr: 0.00908, train_loss: 0.2230, val_loss: 0.2574, val_acc: 0.8865\n","Epoch [24], last_lr: 0.00936, train_loss: 0.1450, val_loss: 0.5996, val_acc: 0.6628\n","Epoch [25], last_lr: 0.00958, train_loss: 0.1903, val_loss: 0.1894, val_acc: 0.9391\n","Epoch [26], last_lr: 0.00976, train_loss: 0.1811, val_loss: 0.6240, val_acc: 0.7434\n","Epoch [27], last_lr: 0.00990, train_loss: 0.2289, val_loss: 2.3106, val_acc: 0.6826\n","Epoch [28], last_lr: 0.00997, train_loss: 0.1756, val_loss: 0.3068, val_acc: 0.8832\n","Epoch [29], last_lr: 0.01000, train_loss: 0.1708, val_loss: 0.4936, val_acc: 0.8947\n","Epoch [30], last_lr: 0.00999, train_loss: 0.1865, val_loss: 0.4644, val_acc: 0.8174\n","Epoch [31], last_lr: 0.00998, train_loss: 0.1377, val_loss: 0.3726, val_acc: 0.8569\n","Epoch [32], last_lr: 0.00995, train_loss: 0.1700, val_loss: 0.2146, val_acc: 0.9293\n","Epoch [33], last_lr: 0.00992, train_loss: 0.1821, val_loss: 0.7997, val_acc: 0.7303\n","Epoch [34], last_lr: 0.00987, train_loss: 0.1714, val_loss: 0.7023, val_acc: 0.8158\n","Epoch [35], last_lr: 0.00982, train_loss: 0.1937, val_loss: 0.5243, val_acc: 0.8569\n","Epoch [36], last_lr: 0.00976, train_loss: 0.1520, val_loss: 0.2978, val_acc: 0.8651\n","Epoch [37], last_lr: 0.00968, train_loss: 0.1752, val_loss: 0.6081, val_acc: 0.7928\n","Epoch [38], last_lr: 0.00960, train_loss: 0.1598, val_loss: 0.7006, val_acc: 0.7895\n","Epoch [39], last_lr: 0.00950, train_loss: 0.1279, val_loss: 0.2169, val_acc: 0.9227\n","Epoch [40], last_lr: 0.00940, train_loss: 0.1412, val_loss: 2.5265, val_acc: 0.5872\n","Epoch [41], last_lr: 0.00929, train_loss: 0.1829, val_loss: 0.3572, val_acc: 0.8240\n","Epoch [42], last_lr: 0.00917, train_loss: 0.1260, val_loss: 0.2498, val_acc: 0.9211\n","Epoch [43], last_lr: 0.00905, train_loss: 0.1340, val_loss: 0.8316, val_acc: 0.6842\n","Epoch [44], last_lr: 0.00891, train_loss: 0.1420, val_loss: 0.1089, val_acc: 0.9523\n","Epoch [45], last_lr: 0.00877, train_loss: 0.1171, val_loss: 0.6657, val_acc: 0.7977\n","Epoch [46], last_lr: 0.00861, train_loss: 0.1548, val_loss: 0.1768, val_acc: 0.9359\n","Epoch [47], last_lr: 0.00846, train_loss: 0.1305, val_loss: 0.7039, val_acc: 0.7780\n","Epoch [48], last_lr: 0.00829, train_loss: 0.1405, val_loss: 0.1881, val_acc: 0.8980\n","Epoch [49], last_lr: 0.00812, train_loss: 0.1247, val_loss: 0.4317, val_acc: 0.8684\n","Epoch [50], last_lr: 0.00794, train_loss: 0.1327, val_loss: 0.1874, val_acc: 0.9457\n","Epoch [51], last_lr: 0.00775, train_loss: 0.1115, val_loss: 0.1472, val_acc: 0.9622\n","Epoch [52], last_lr: 0.00756, train_loss: 0.1125, val_loss: 0.1728, val_acc: 0.9359\n","Epoch [53], last_lr: 0.00737, train_loss: 0.0992, val_loss: 0.2028, val_acc: 0.9194\n","Epoch [54], last_lr: 0.00717, train_loss: 0.0970, val_loss: 0.1081, val_acc: 0.9704\n","Epoch [55], last_lr: 0.00697, train_loss: 0.1140, val_loss: 0.1883, val_acc: 0.9408\n","Epoch [56], last_lr: 0.00676, train_loss: 0.1032, val_loss: 0.1676, val_acc: 0.9457\n","Epoch [57], last_lr: 0.00655, train_loss: 0.1200, val_loss: 1.7447, val_acc: 0.6135\n","Epoch [58], last_lr: 0.00633, train_loss: 0.1212, val_loss: 0.1136, val_acc: 0.9737\n","Epoch [59], last_lr: 0.00611, train_loss: 0.0805, val_loss: 0.2116, val_acc: 0.9030\n","Epoch [60], last_lr: 0.00589, train_loss: 0.1019, val_loss: 0.1709, val_acc: 0.9474\n","Epoch [61], last_lr: 0.00567, train_loss: 0.0985, val_loss: 0.3311, val_acc: 0.8586\n","Epoch [62], last_lr: 0.00545, train_loss: 0.1176, val_loss: 0.1559, val_acc: 0.9572\n","Epoch [63], last_lr: 0.00522, train_loss: 0.0891, val_loss: 0.4242, val_acc: 0.8849\n","Epoch [64], last_lr: 0.00500, train_loss: 0.0940, val_loss: 0.4266, val_acc: 0.8766\n","Epoch [65], last_lr: 0.00478, train_loss: 0.0889, val_loss: 0.2254, val_acc: 0.8997\n","Epoch [66], last_lr: 0.00455, train_loss: 0.0731, val_loss: 0.2007, val_acc: 0.9112\n","Epoch [67], last_lr: 0.00433, train_loss: 0.1225, val_loss: 0.1237, val_acc: 0.9638\n","Epoch [68], last_lr: 0.00411, train_loss: 0.0778, val_loss: 0.1655, val_acc: 0.9145\n","Epoch [69], last_lr: 0.00389, train_loss: 0.0626, val_loss: 0.2306, val_acc: 0.9178\n","Epoch [70], last_lr: 0.00367, train_loss: 0.0619, val_loss: 0.1398, val_acc: 0.9720\n","Epoch [71], last_lr: 0.00345, train_loss: 0.0582, val_loss: 0.1203, val_acc: 0.9704\n","Epoch [72], last_lr: 0.00324, train_loss: 0.0854, val_loss: 0.1762, val_acc: 0.9605\n","Epoch [73], last_lr: 0.00303, train_loss: 0.0736, val_loss: 0.1207, val_acc: 0.9671\n","Epoch [74], last_lr: 0.00283, train_loss: 0.0674, val_loss: 0.1512, val_acc: 0.9704\n","Epoch [75], last_lr: 0.00263, train_loss: 0.0721, val_loss: 0.1565, val_acc: 0.9556\n","Epoch [76], last_lr: 0.00244, train_loss: 0.0598, val_loss: 0.3206, val_acc: 0.9030\n","Epoch [77], last_lr: 0.00225, train_loss: 0.0626, val_loss: 0.1931, val_acc: 0.9605\n","Epoch [78], last_lr: 0.00206, train_loss: 0.0650, val_loss: 0.1855, val_acc: 0.9655\n","Epoch [79], last_lr: 0.00188, train_loss: 0.0606, val_loss: 0.1089, val_acc: 0.9704\n","Epoch [80], last_lr: 0.00171, train_loss: 0.0578, val_loss: 0.1140, val_acc: 0.9688\n","Epoch [81], last_lr: 0.00154, train_loss: 0.0486, val_loss: 0.2117, val_acc: 0.9128\n","Epoch [82], last_lr: 0.00139, train_loss: 0.0408, val_loss: 0.1918, val_acc: 0.9638\n","Epoch [83], last_lr: 0.00123, train_loss: 0.0454, val_loss: 0.1730, val_acc: 0.9161\n","Epoch [84], last_lr: 0.00109, train_loss: 0.0424, val_loss: 0.1572, val_acc: 0.9704\n","Epoch [85], last_lr: 0.00095, train_loss: 0.0441, val_loss: 0.1577, val_acc: 0.9671\n","Epoch [86], last_lr: 0.00083, train_loss: 0.0429, val_loss: 0.2005, val_acc: 0.9622\n","Epoch [87], last_lr: 0.00071, train_loss: 0.0377, val_loss: 0.2024, val_acc: 0.9638\n","Epoch [88], last_lr: 0.00060, train_loss: 0.0379, val_loss: 0.2207, val_acc: 0.9622\n","Epoch [89], last_lr: 0.00050, train_loss: 0.0410, val_loss: 0.2044, val_acc: 0.9622\n","Epoch [90], last_lr: 0.00040, train_loss: 0.0328, val_loss: 0.1912, val_acc: 0.9655\n","Epoch [91], last_lr: 0.00032, train_loss: 0.0356, val_loss: 0.1779, val_acc: 0.9655\n","Epoch [92], last_lr: 0.00024, train_loss: 0.0287, val_loss: 0.1999, val_acc: 0.9638\n","Epoch [93], last_lr: 0.00018, train_loss: 0.0322, val_loss: 0.1853, val_acc: 0.9655\n","Epoch [94], last_lr: 0.00013, train_loss: 0.0302, val_loss: 0.1909, val_acc: 0.9638\n","Epoch [95], last_lr: 0.00008, train_loss: 0.0291, val_loss: 0.1741, val_acc: 0.9671\n","Epoch [96], last_lr: 0.00005, train_loss: 0.0320, val_loss: 0.1824, val_acc: 0.9655\n","Epoch [97], last_lr: 0.00002, train_loss: 0.0334, val_loss: 0.1972, val_acc: 0.9638\n","Epoch [98], last_lr: 0.00001, train_loss: 0.0297, val_loss: 0.1817, val_acc: 0.9638\n","Epoch [99], last_lr: 0.00000, train_loss: 0.0338, val_loss: 0.2002, val_acc: 0.9638\n","CPU times: user 32min 11s, sys: 1min 5s, total: 33min 17s\n","Wall time: 40min 41s\n"]}],"source":["%%time\n","history = fit_one_cycle(\n","  epochs,\n","  max_lr,\n","  model,\n","  train_loader,\n","  val_loader,\n","  grad_clip=grad_clip,\n","  weight_decay=1e-4,\n","  opt_func=opt_func\n",")"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1704837975352,"user":{"displayName":"Jean Carlos Soncco Lupa","userId":"15843525091642984262"},"user_tz":300},"id":"pogEdgyLxXvE"},"outputs":[],"source":["torch.save(model.state_dict(), f'models/plant_disease_aug_googlenet_{epochs}.pth')"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1704837975352,"user":{"displayName":"Jean Carlos Soncco Lupa","userId":"15843525091642984262"},"user_tz":300},"id":"cLiJ3rNuC04p"},"outputs":[],"source":["# clear nvidia cache\n","import torch, gc\n","gc.collect()\n","torch.cuda.empty_cache()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNz0m4RIr3rxdAOMCXOzWzg","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
